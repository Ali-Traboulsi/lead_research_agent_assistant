{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-23T17:01:31.828970Z",
     "start_time": "2024-11-23T17:01:31.814884Z"
    }
   },
   "source": [
    "import uuid\n",
    "\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver \n",
    "from langchain_cohere import ChatCohere"
   ],
   "outputs": [],
   "execution_count": 240
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:00.129865Z",
     "start_time": "2024-11-23T08:47:59.731950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    # Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "model = ChatCohere(cohere_api_key=\"QTIAR07ZVhcAVAPrUTHQozivAbRFhmhdoWwPsclg\", temperature=0.1)\n"
   ],
   "id": "d0aa35a351c9876f",
   "outputs": [],
   "execution_count": 173
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f4f6897872ee368a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:00.139193Z",
     "start_time": "2024-11-23T08:48:00.136420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}"
   ],
   "id": "894814d127149d53",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:00.157020Z",
     "start_time": "2024-11-23T08:48:00.152412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define workflow nodes\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)"
   ],
   "id": "44ac940177ebe59f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2753632aa10>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 175
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9ea29d7e120b3712"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:00.185620Z",
     "start_time": "2024-11-23T08:48:00.182536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ],
   "id": "860fe2746b756235",
   "outputs": [],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:00.197019Z",
     "start_time": "2024-11-23T08:48:00.194225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate a unique thread ID for conversation management\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}"
   ],
   "id": "b667a121835cc171",
   "outputs": [],
   "execution_count": 177
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:00.212493Z",
     "start_time": "2024-11-23T08:48:00.210157Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2ee3ec69020547e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:00.216853Z",
     "start_time": "2024-11-23T08:48:00.214506Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ff2efbc311331491",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:00.228025Z",
     "start_time": "2024-11-23T08:48:00.225118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "field_extraction_prompt = \"\"\"\n",
    "# Role\n",
    "\n",
    "You are a Data Extraction Agent.\n",
    "\n",
    "# Objective\n",
    "\n",
    "Your objective is to identify and extract specific business details from provided text, including industry, company size, location, and job roles. You should construct a full `user_info` dictionary with these details and indicate if there are any missing fields.\n",
    "\n",
    "# Context\n",
    "\n",
    "The extracted information will help provide structured business data from raw text inputs, which will support various business and analytical operations.\n",
    "\n",
    "# SOP\n",
    "\n",
    "1. Analyze the user-provided text in the variable user_input.\n",
    "\n",
    "2. Construct and return the following dictionary:\n",
    "\n",
    "{ \"company_size\": null or employee range (e.g., \"1-10\", \"500+\"), \"industry\": null or specific sector (e.g., \"Tech\", \"Healthcare\"), \"location\": null or specific location (e.g., city or country), \"job_roles\": [] or list of job titles mentioned }\n",
    "\n",
    "3. In addition to the dictionary, provide a list of fields that are still missing or mention that all fields are complete.\n",
    "\n",
    "# Examples\n",
    "\n",
    "**Example 1:**\n",
    "\n",
    "**Input:**\n",
    "\n",
    "\"Text describing a tech company based in New York with 200+ employees and titles like CEO, Data Scientist, and Product Manager.\"\n",
    "\n",
    "**Output:**\n",
    "\n",
    "{\n",
    "    \"user_info\": {\n",
    "        \"company_size\": \"200+\",\n",
    "        \"industry\": \"Tech\",\n",
    "        \"location\": \"New York\",\n",
    "        \"job_roles\": [\"CEO\", \"Data Scientist\", \"Product Manager\"]\n",
    "    },\n",
    "    \"missing_fields\": []\n",
    "}\n",
    "\n",
    "**Example 2:**\n",
    "\n",
    "**Input:**\n",
    "\n",
    "\"Description of a healthcare firm in Dubai without specific company size or job titles mentioned.\"\n",
    "\n",
    "**Output:**\n",
    "\n",
    "{\n",
    "    \"user_info\": {\n",
    "        \"company_size\": null,\n",
    "        \"industry\": \"Healthcare\",\n",
    "        \"location\": \"Dubai\",\n",
    "        \"job_roles\": []\n",
    "    },\n",
    "    \"missing_fields\": [\"company_size\", \"job_roles\"]\n",
    "}\n",
    "\n",
    "**Example 3:**\n",
    "\n",
    "**Input:**\n",
    "\n",
    "\"A small tech startup with less than 10 employees, located in San Francisco. The team includes a CEO and a CTO.\"\n",
    "\n",
    "**Output:**\n",
    "\n",
    "{\n",
    "    \"user_info\": {\n",
    "        \"company_size\": \"1-10\",\n",
    "        \"industry\": \"Tech\",\n",
    "        \"location\": \"San Francisco\",\n",
    "        \"job_roles\": [\"CEO\", \"CTO\"]\n",
    "    },\n",
    "    \"missing_fields\": []\n",
    "}\n",
    "\"\"\""
   ],
   "id": "d84b729eabeef5f3",
   "outputs": [],
   "execution_count": 178
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extract user fields:\n",
   "id": "51261c569372d48a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:00.249051Z",
     "start_time": "2024-11-23T08:48:00.246957Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5fbdd77185121ec8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:00.260009Z",
     "start_time": "2024-11-23T08:48:00.257064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "REQUIRED_FIELDS = {\"company_size\", \"industry\", \"job_roles\", \"location\"}\n",
    "\n",
    "# Global dictionary to keep track of extracted fields and missing fields\n",
    "recorded_fields = {\n",
    "    \"user_info\": {},\n",
    "    \"missing_fields\": list(REQUIRED_FIELDS)  # Initially, all fields are required\n",
    "}"
   ],
   "id": "73478c784817f89f",
   "outputs": [],
   "execution_count": 179
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:00.275567Z",
     "start_time": "2024-11-23T08:48:00.272656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_user_fields(message, model):\n",
    "    prompt = field_extraction_prompt + f\"\\nInput: {message}\\nResponse format (JSON): {{'company_size': '', 'industry': '', 'job_roles': '', 'location': ''}}\"\n",
    "        \n",
    "    try: \n",
    "        llm_response = model.predict(prompt) \n",
    "        llm_extracted_data = json.loads(llm_response)\n",
    "        print(llm_extracted_data)\n",
    "        return llm_extracted_data\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing key in template formatting: {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to decode JSON response: {e}\")\n"
   ],
   "id": "903137cf2cfdec8f",
   "outputs": [],
   "execution_count": 180
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:00.289699Z",
     "start_time": "2024-11-23T08:48:00.287710Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "27cb72ace8b69288",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test the extraction\n",
    "user_input = \"A startup\"\n",
    "extracted_data = extract_user_fields(user_input, model)\n",
    "\n",
    "print(\"Extracted Data:\", extracted_data)\n",
    "\n",
    "\n",
    "# Define the required fields\n",
    "required_fields = ['industry', 'company_size', 'location', 'job_roles']\n",
    "\n",
    "print(\"Missing Fields:\", extracted_data[\"missing_fields\"])\n"
   ],
   "id": "5a2d24e0fc0aa8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:02.488181Z",
     "start_time": "2024-11-23T08:48:02.484456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Memory initialization\n",
    "memory_state = {\n",
    "    \"conversation\": [],\n",
    "    \"extracted_data\": {\"company_size\": None, \"industry\": None, \"job_roles\": None, \"location\": None},\n",
    "    \"missing_fields\": [\"company_size\", \"industry\", \"job_roles\", \"location\"],\n",
    "}\n",
    "\n",
    "def update_memory_with_response(llm_memory_state, llm_extracted_data):\n",
    "    for field, value in llm_extracted_data.items():\n",
    "        if value in llm_memory_state[\"extracted_data\"]:\n",
    "            llm_memory_state[\"extracted_data\"][field] = value\n",
    "            llm_memory_state[\"missing_fields\"].remove(field)\n",
    "    return llm_memory_state\n",
    "\n",
    "\n"
   ],
   "id": "805a060bc3255b5c",
   "outputs": [],
   "execution_count": 182
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T11:56:57.987152Z",
     "start_time": "2024-11-23T11:56:57.979181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dynamic_prompt(user_input, missing_fields):\n",
    "    prompt = f\"Extract the following details: {', '.join(missing_fields)}.\\n\"\n",
    "    prompt += f\"Input: {user_input}\\nResponse format (JSON): {{'company_size': '', 'industry': '', 'job_roles': '', 'location': ''}}\"\n",
    "    return prompt\n"
   ],
   "id": "ba1707b7f19327e6",
   "outputs": [],
   "execution_count": 194
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a5e1d30ecf5ec969"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T13:28:23.706608Z",
     "start_time": "2024-11-23T13:28:23.683790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def feedback_loop(model, memory_state):\n",
    "    \"\"\"Iterative dialog with the user to complete missing fields.\"\"\"\n",
    "    print(\"Agent: Let's gather some details about your target.\")\n",
    "    \n",
    "    while memory_state[\"missing_fields\"]:\n",
    "        missing_fields = memory_state[\"missing_fields\"]    \n",
    "    \n",
    "        dynamic_prompt = create_dynamic_prompt(\"\", missing_fields)\n",
    "\n",
    "        # Ask the user for the missing field\n",
    "        user_input = input(f\"Agent: Please provide details about {', '.join(missing_fields)}.\\nUser: \")\n",
    "        \n",
    "        # Update the prompt with user input\n",
    "        full_prompt = create_dynamic_prompt(user_input, missing_fields)\n",
    "\n",
    "        # Extract the user input\n",
    "        extracted_data = extract_user_fields(full_prompt, model)\n",
    "        \n",
    "        # Update the memory state\n",
    "        memory_state = update_memory_with_response(memory_state, extracted_data)\n",
    "        \n",
    "        # Log the conversation\n",
    "        memory_state[\"conversation\"].append({\"role\": \"user\", \"content\": user_input})\n",
    "        memory_state[\"conversation\"].append({\"role\": \"agent\", \"content\": f\"Extracted: {extracted_data}\"})\n",
    "        print(\"Memory State:\", memory_state)\n",
    "        print(\"Agent: Thank you! All fields are complete.\")\n",
    "\n",
    "    return memory_state"
   ],
   "id": "f4ded6f865071fd7",
   "outputs": [],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T13:28:48.436161Z",
     "start_time": "2024-11-23T13:28:47.375223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "validation_prompt = create_dynamic_prompt(user_input, [\"company_size\", \"location\"])\n",
    "validation_response = model.predict(validation_prompt)"
   ],
   "id": "cb70eb81d1595d4a",
   "outputs": [],
   "execution_count": 196
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "extracted_data = extract_user_fields(user_input, model)\n",
    "refined_query = create_dynamic_prompt(\"\", [\"industry\", \"location\"])"
   ],
   "id": "a5790afc7eb00e17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Query Matching and storage",
   "id": "ecd17c63b81b4fcc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T14:37:51.085650Z",
     "start_time": "2024-11-23T14:37:51.081277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def query_matching(user_query, llm_vector_store, llm_responses, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Matches a user query to the most similar predefined question in the vector store.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The query from the user.\n",
    "        llm_vector_store (FAISS): Vector store containing embeddings of predefined questions.\n",
    "        llm_responses (dict): Dictionary mapping predefined questions to responses.\n",
    "        threshold (float): Minimum similarity score to consider a match.\n",
    "\n",
    "    Returns:\n",
    "        str: The matched response or a fallback response if no match is found.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        matches = vector_store.similarity_search_with_score(user_query, k=1)\n",
    "        \n",
    "        if not matches:\n",
    "            return \"Sorry, I couldn't find a relevant match for your query.\"\n",
    "\n",
    "        # Retrieve the best match and its similarity score\n",
    "        best_match, score = matches[0]  # Unpack the first result (document, score)\n",
    "        \n",
    "        print(f'best_match:{best_match.page_content}')\n",
    "        print(f'score:{score}')\n",
    "        \n",
    "        if score >= threshold:\n",
    "            question= best_match.page_content    \n",
    "            return llm_responses.get(question, \"No Predefined response found\")\n",
    "        else:\n",
    "            return \"Sorry, your query doesn't match any known question closely enough.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query matching: {e}\")\n",
    "        return \"No match found.\""
   ],
   "id": "f80330a0796fedec",
   "outputs": [],
   "execution_count": 225
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T14:37:54.487928Z",
     "start_time": "2024-11-23T14:37:53.308659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "qa_dataset = pd.read_csv('./customer_leads_agent_qa_data_csv.csv', delimiter=',').to_dict(orient='records')\n",
    "\n",
    "# Convert questions into embeddings\n",
    "embeddings = CohereEmbeddings(cohere_api_key=\"QTIAR07ZVhcAVAPrUTHQozivAbRFhmhdoWwPsclg\", model=\"embed-english-light-v3.0\" )\n",
    "\n",
    "questions = [item[\"prompt_text\"] for item in qa_dataset]\n",
    "vector_store = FAISS.from_texts(questions, embeddings)\n",
    "    \n",
    "# Map each vector to its expected response\n",
    "responses = {item[\"prompt_text\"]: item[\"expected_response\"] for item in qa_dataset}\n"
   ],
   "id": "fa896d452afe896e",
   "outputs": [],
   "execution_count": 226
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T14:37:55.331496Z",
     "start_time": "2024-11-23T14:37:55.069018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# User's input query\n",
    "user_query = \"What unique value do they offer?\"\n",
    "\n",
    "# Query matching\n",
    "response = query_matching(user_query, vector_store, responses, threshold=0.3)\n",
    "\n",
    "# Print the response\n",
    "print(\"Agent's Response:\", response)\n"
   ],
   "id": "b93c1cbfbb752dc3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_match:\"What unique value do they offer?\"\n",
      "score:0.7451424598693848\n",
      "Agent's Response: \"They offer unique value in areas like [strengths].\"\n"
     ]
    }
   ],
   "execution_count": 227
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:03.737688Z",
     "start_time": "2024-11-23T08:48:03.734516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def merge_memory(current_memory, new_memory):\n",
    "    for key, value in new_memory.items():\n",
    "        if isinstance(value, dict) and key in current_memory:\n",
    "            # Recursive merge for nested dictionaries\n",
    "            merge_memory(current_memory[key], value)\n",
    "        else:\n",
    "            # Update scalar values or new keys\n",
    "            current_memory[key] = value"
   ],
   "id": "fcc2c1cb1c618145",
   "outputs": [],
   "execution_count": 186
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:03.747193Z",
     "start_time": "2024-11-23T08:48:03.744526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def handle_no_match(llm_memory_state, llm_model):\n",
    "#     \"\"\"Fallback to feedback loop when no match is found.\"\"\"\n",
    "#     print(\"Agent: I couldn't find a predefined match. Let's refine your query.\")\n",
    "#     llm_memory_state, llm_extracted_data = feedback_loop(memory_state=memory_state, model=llm_model)\n",
    "#     return llm_memory_state, llm_extracted_data\n"
   ],
   "id": "dec3fac87074e43a",
   "outputs": [],
   "execution_count": 187
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T14:00:53.882686Z",
     "start_time": "2024-11-23T14:00:53.769074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "feedback_prompt_template = f\"\"\"\n",
    "# Role\n",
    "You are a Data Extraction Agent responsible for collecting specific business details iteratively.\n",
    "\n",
    "# Objective\n",
    "Your task is to:\n",
    "1. Update the `memory_state` dictionary with any new information extracted from `user_input`.\n",
    "2. Identify any fields still missing and construct a `next_prompt` to ask the user for these details.\n",
    "3. If all required fields are complete, indicate this in the `next_prompt`.\n",
    "\n",
    "# Memory State: {memory_state}\n",
    "\n",
    "# User Input: {user_input}\n",
    "\n",
    "# Response Format\n",
    "Respond in the following JSON format:\n",
    "{\n",
    "    \"updated_memory\": {memory_state},\n",
    "    \"next_prompt\": \"string\"\n",
    "}\n",
    "\"\"\"\n"
   ],
   "id": "28b20a976c4c154b",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid format specifier ' {'conversation': [], 'extracted_data': {'company_size': None, 'industry': None, 'job_roles': None, 'location': None}, 'missing_fields': ['company_size', 'industry', 'job_roles', 'location']},\n    \"next_prompt\": \"string\"\n' for object of type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[202], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m feedback_prompt_template \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124m# Role\u001B[39m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;124mYou are a Data Extraction Agent responsible for collecting specific business details iteratively.\u001B[39m\n\u001B[0;32m      6\u001B[0m \n\u001B[0;32m      7\u001B[0m \u001B[38;5;124m# Objective\u001B[39m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;124mYour task is to:\u001B[39m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;124m1. Update the `memory_state` dictionary with any new information extracted from `user_input`.\u001B[39m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;124m2. Identify any fields still missing and construct a `next_prompt` to ask the user for these details.\u001B[39m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;124m3. If all required fields are complete, indicate this in the `next_prompt`.\u001B[39m\n\u001B[0;32m     12\u001B[0m \n\u001B[0;32m     13\u001B[0m \u001B[38;5;124m# Memory State: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmemory_state\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \n\u001B[0;32m     15\u001B[0m \u001B[38;5;124m# User Input: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00muser_input\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \n\u001B[0;32m     17\u001B[0m \u001B[38;5;124m# Response Format\u001B[39m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;124mRespond in the following JSON format:\u001B[39m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;132;01m{\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdated_memory\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmemory_state\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;124m    \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext_prompt\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstring\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;132;01m}\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n",
      "\u001B[1;31mValueError\u001B[0m: Invalid format specifier ' {'conversation': [], 'extracted_data': {'company_size': None, 'industry': None, 'job_roles': None, 'location': None}, 'missing_fields': ['company_size', 'industry', 'job_roles', 'location']},\n    \"next_prompt\": \"string\"\n' for object of type 'str'"
     ]
    }
   ],
   "execution_count": 202
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:48:03.776920100Z",
     "start_time": "2024-11-23T07:50:29.086302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def model_driven_feedback_loop(fcn_user_input, fcn_memory_state, model):\n",
    "    \"\"\"\n",
    "    Handles feedback loop by using the model to manage memory and prompts dynamically.\n",
    "    \n",
    "    Args:\n",
    "        fcn_user_input (str): The current user input.\n",
    "        fcn_memory_state (dict): Dictionary containing collected data and missing fields.\n",
    "        model (ChatOpenAI): Language model instance.\n",
    "    \n",
    "    Returns:\n",
    "        updated_memory (dict): Updated memory state with new extracted data.\n",
    "        next_prompt (str): Next prompt for the user.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare memory for the prompt\n",
    "        memory_state_str = json.dumps(fcn_memory_state, indent=4)\n",
    "        updated_memory = fcn_memory_state.get(\"updated_memory\", {})\n",
    "        updated_memory_str = json.dumps(updated_memory, indent=4)\n",
    "\n",
    "        # Construct the prompt\n",
    "        prompt = f\"\"\"\n",
    "        Role: You are a Data Extraction Agent responsible for collecting specific business details iteratively. \n",
    "        # Objective\n",
    "        Your task is to:\n",
    "        1. Update the `memory_state` dictionary with any new information extracted from `user_input`.\n",
    "        2. Identify any fields still missing and construct a `next_prompt` to ask the user for these details.\n",
    "        3. If all required fields are complete, indicate this in the `next_prompt`.\n",
    "\n",
    "        # Memory State:\n",
    "        {memory_state_str}\n",
    "\n",
    "        # User Input:\n",
    "        {fcn_user_input}\n",
    "\n",
    "        # Response Format\n",
    "        Respond in the following JSON format:\n",
    "        {{\n",
    "            \"updated_memory\": {updated_memory_str},\n",
    "            \"next_prompt\": \"string\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Debugging: Print the constructed prompt\n",
    "        print(f\"Prompt sent to model: {prompt}\")\n",
    "        \n",
    "        # Generate the response\n",
    "        response = model.predict(prompt)\n",
    "        print(f\"Response from model: {response}\")\n",
    "\n",
    "        # Parse the response\n",
    "        response_data = json.loads(response)\n",
    "        \n",
    "        # Update memory\n",
    "        fcn_updated_memory = copy.deepcopy(fcn_memory_state)\n",
    "        merge_memory(fcn_updated_memory, response_data.get(\"updated_memory\", {}))\n",
    "        \n",
    "        \n",
    "        # Debugging: Check memory updates\n",
    "        print(\"Original memory state:\", json.dumps(fcn_memory_state, indent=4))\n",
    "        print(\"Response data:\", json.dumps(response_data, indent=4))\n",
    "        print(\"Updated memory state:\", json.dumps(fcn_updated_memory, indent=4))\n",
    "\n",
    "        # Get the next prompt\n",
    "        fcn_next_prompt = response_data.get(\"next_prompt\", \"All required fields are complete.\")\n",
    "\n",
    "        return fcn_updated_memory, fcn_next_prompt\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON response: {e}\")\n",
    "        return fcn_memory_state, \"An error occurred. Please try again.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return fcn_memory_state, \"An error occurred. Please try again.\""
   ],
   "id": "72a9aabc4061128e",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "30bb8145ca48851b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T14:53:47.153614Z",
     "start_time": "2024-11-23T14:53:47.144120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_inputs_example = [\n",
    "    \"I am targeting companies in San Francisco.\",\n",
    "    \"These companies should have 500+ employees.\",\n",
    "    \"I want to reach CTOs and Product Managers.\"\n",
    "]\n",
    "\n",
    "memory_state_example = {\n",
    "    \"updated_memory\": {\n",
    "        \"company_size\": None,\n",
    "        \"industry\": None,\n",
    "        \"location\": None,\n",
    "        \"job_roles\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "\n"
   ],
   "id": "7c51be243b1d34e",
   "outputs": [],
   "execution_count": 228
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "577eb830191912df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Iterative feedback loop\n",
    "for user_input in user_inputs_example:\n",
    "    memory_state_example, next_prompt = model_driven_feedback_loop(\n",
    "        fcn_user_input=user_input,\n",
    "        fcn_memory_state=memory_state_example,\n",
    "        model=model\n",
    "    )\n",
    "    print(f\"Agent: {next_prompt}\")\n",
    "    print(f\"Updated Memory State: {json.dumps(memory_state_example, indent=4)}\")\n",
    "    if \"All required fields are complete\" in next_prompt:\n",
    "        break\n"
   ],
   "id": "5c3b165deb504264",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4278b5e8a410195d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T16:10:28.668463Z",
     "start_time": "2024-11-23T16:10:28.621190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chat_with_model(model_fcn, memory_state_fcn):\n",
    "    \"\"\"\n",
    "    Simulates a chat between the user and the AI model using the model-driven feedback loop.\n",
    "    \n",
    "    Args:\n",
    "        model_fcn (ChatOpenAI): The AI model instance.\n",
    "        memory_state_fcn (dict): Tracks extracted data and missing fields.\n",
    "\n",
    "    Returns:\n",
    "        None: Continuously interacts with the user until all fields are collected.\n",
    "    \"\"\"\n",
    "    print(\"Agent: Let's gather the necessary details. Feel free to provide information step by step.\")\n",
    "    \n",
    "    debug_mode = True  # Set this flag to False in production    \n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "                 \n",
    "            if memory_state_fcn[\"missing_fields\"]:\n",
    "                next_prompt_fcn = f\"Please provide the {memory_state_fcn['missing_fields'][0]}.\"\n",
    "            else:\n",
    "                next_prompt_fcn = \"All required fields are complete. Do you need any further assistance?\"\n",
    " \n",
    "       \n",
    "            # Display the AI's next prompt to the user\n",
    "            # Display the AI's next prompt to the user\n",
    "            print(f\"Agent: {next_prompt_fcn}\")\n",
    "\n",
    "\n",
    "            # Get user input\n",
    "            user_input = input(\"User: \")\n",
    "            \n",
    "                        # Exit condition\n",
    "            if user_input.lower() in [\"exit\", \"quit\", \"done\"]:\n",
    "                print(\"Agent: Thank you for the information. Goodbye!\")\n",
    "                break\n",
    "                \n",
    "                \n",
    "            if not user_input.strip():\n",
    "                print(\"Agent: I didn't catch that. Could you provide more details?\")\n",
    "                continue\n",
    "\n",
    "            # Match the user query to a predefined response\n",
    "            response = query_matching(user_input, vector_store, responses)\n",
    "            \n",
    "            if response:\n",
    "                print(\"Agent:\", response)\n",
    "            else:\n",
    "                # Run the feedback loop with the user's input\n",
    "                memory_state_fcn, next_prompt_fcn = model_driven_feedback_loop(\n",
    "                    fcn_user_input=user_input,\n",
    "                    fcn_memory_state=memory_state_fcn,\n",
    "                    model=model_fcn\n",
    "                )\n",
    "        \n",
    "        \n",
    "            # Debugging: Log the updated memory state if debugging is enabled\n",
    "            if debug_mode:\n",
    "                print(f\"Updated Memory State: {json.dumps(memory_state_fcn, indent=4)}\")\n",
    "\n",
    "            # Check if all fields are complete\n",
    "            if \"All required fields are complete\" in next_prompt_fcn:\n",
    "                print(f\"Agent: {next_prompt_fcn}\")\n",
    "                print(f\"Final Memory State: {json.dumps(memory_state_fcn, indent=4)}\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Agent: I encountered an issue. Please try again.\")\n",
    "            break"
   ],
   "id": "14e36ad4228d95d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is your name?\n",
      "User input: Alice\n",
      "All information has been collected.\n",
      "{'missing_fields': [], 'next_prompt': 'Thank you! Any more information to provide?', 'user_name': 'Alice'}\n"
     ]
    }
   ],
   "execution_count": 238
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3883207902dece1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:53:33.269092Z",
     "start_time": "2024-11-23T08:53:33.265879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_agent():\n",
    "    \"\"\"\n",
    "    Initializes the model and memory state for the chat agent.\n",
    "    Returns:\n",
    "        model_fcn (ChatCohere): The AI model instance.\n",
    "        memory_state_fcn (dict): The initial memory state.\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    model_fcn = ChatCohere(cohere_api_key=\"QTIAR07ZVhcAVAPrUTHQozivAbRFhmhdoWwPsclg\", temperature=0.1)\n",
    "\n",
    "    # Initialize memory state\n",
    "    memory_state_fcn = {\n",
    "        \"updated_memory\": {\n",
    "            \"company_size\": None,\n",
    "            \"industry\": None,\n",
    "            \"location\": None,\n",
    "            \"job_roles\": []\n",
    "        },\n",
    "        \"next_prompt\": \"How may I help you today?\"\n",
    "    }\n",
    "\n",
    "    return model_fcn, memory_state_fcn\n"
   ],
   "id": "7b0d93a8076834c6",
   "outputs": [],
   "execution_count": 191
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T16:07:14.504625Z",
     "start_time": "2024-11-23T16:07:14.501826Z"
    }
   },
   "cell_type": "code",
   "source": "initial_memory_state = {\"missing_fields\": [\"user_name\"], \"next_prompt\": \"What is your name?\"}\n",
   "id": "73c5bb710b79a044",
   "outputs": [],
   "execution_count": 236
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T17:16:40.120090Z",
     "start_time": "2024-11-23T17:16:40.113145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":    \n",
    "   chat_with_model(model_fcn=model, memory_state_fcn=initial_memory_state)\n"
   ],
   "id": "1efc81ac9e5dd9dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All information has been collected.\n"
     ]
    }
   ],
   "execution_count": 241
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T17:17:04.003857Z",
     "start_time": "2024-11-23T17:17:03.997243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_dynamic_prompt(missing_field):\n",
    "    prompts = {\n",
    "        \"industry\": \"What industry are you working in?\",\n",
    "        \"company_size\": \"Can you tell me your company size?\",\n",
    "        \"location\": \"Where is your company located?\",\n",
    "        \"job_roles\": \"What are the key job roles in your company?\",\n",
    "    }\n",
    "    return prompts.get(missing_field, \"Can you provide more details?\")\n"
   ],
   "id": "9fc348bb8eb738b",
   "outputs": [],
   "execution_count": 242
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T17:17:08.089475Z",
     "start_time": "2024-11-23T17:17:08.080482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chat_flow():\n",
    "    while memory_state[\"missing_fields\"]:\n",
    "        # Pick the first missing field\n",
    "        missing_field = memory_state[\"missing_fields\"][0]\n",
    "        \n",
    "        # Generate a prompt for the user\n",
    "        prompt = get_dynamic_prompt(missing_field)\n",
    "        print(prompt)  # This is where the user sees the question\n",
    "        \n",
    "        # Simulate user response\n",
    "        user_response = input(\"Your response: \")  # Replace with actual input logic\n",
    "\n",
    "        # Update memory with user response\n",
    "        if user_response.strip():\n",
    "            memory_state[\"extracted_data\"][missing_field] = user_response\n",
    "            memory_state[\"missing_fields\"].remove(missing_field)\n",
    "        else:\n",
    "            print(\"Invalid response, please provide more details.\")\n",
    "\n",
    "    print(\"All fields have been filled:\")\n",
    "    print(memory_state[\"extracted_data\"])\n"
   ],
   "id": "268a53b5a05a7ffb",
   "outputs": [],
   "execution_count": 243
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a85450e2458c3581"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
